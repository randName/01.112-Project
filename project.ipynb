{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Design Project\n",
    "\n",
    "Entity labelling of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from utils import load_file, get_entities, compare_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Estimation of emission parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_estimate(train):\n",
    "    # count emission y -> x\n",
    "    counts_yx = defaultdict(int)\n",
    "    \n",
    "    # count occurence of y\n",
    "    counts_y = defaultdict(int)\n",
    "    \n",
    "    xs = set()\n",
    "    for line in train:\n",
    "        for tok in line:\n",
    "            x, y = tok\n",
    "            xs.add(x)\n",
    "            counts_y[y] += 1\n",
    "            counts_yx[(y, x)] += 1\n",
    "    \n",
    "    e = {}\n",
    "    for y, count_y in counts_y.items():\n",
    "        for x in xs:\n",
    "            e[(x, y)] = counts_yx[(y, x)]/count_y\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_estimate2(train, k=3):\n",
    "    # count emission of y -> x\n",
    "    counts_yx = defaultdict(int)\n",
    "    \n",
    "    # count occurence of y\n",
    "    counts_y = defaultdict(int)\n",
    "    \n",
    "    xs = set()\n",
    "    for line in train:\n",
    "        for tok in line:\n",
    "            x, y = tok\n",
    "            x = x.lower()\n",
    "            xs.add(x)\n",
    "            counts_y[y] += 1\n",
    "            counts_yx[(y, x)] += 1\n",
    "    \n",
    "    ys = counts_y.keys()\n",
    "    unknowns = set()\n",
    "    \n",
    "    # replace x with unk if it appears less than k times\n",
    "    for x in xs:\n",
    "        if sum(counts_yx[(y, x)] for y in ys) >= k:\n",
    "            continue\n",
    "        unknowns.add(x)\n",
    "        for y in ys:\n",
    "            counts_yx[(y, '#UNK#')] += counts_yx.pop((y, x), 0)\n",
    "            \n",
    "    xs -= unknowns\n",
    "    xs.add('#UNK#')\n",
    "    \n",
    "    e = {}\n",
    "    for y, count_y in counts_y.items():\n",
    "        for x in xs:\n",
    "            count_yx = counts_yx[(y, x)]\n",
    "            if count_yx:\n",
    "                e[(x, y)] = count_yx/count_y\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_analyse(emissions, dev, output):\n",
    "    xs, ys = tuple(set(i) for i in zip(*emissions.keys()))\n",
    "    with open(output, 'w') as f:\n",
    "        for line in dev:\n",
    "            for tok in line:\n",
    "                x = tok[0].lower()\n",
    "                if x not in xs:\n",
    "                    x = '#UNK#'\n",
    "                ym = max((emissions.get((x, y), 0), y) for y in ys)[1]\n",
    "                print(\"%s %s\" % (tok[0], ym), file=f)\n",
    "            print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ('EN', 'SG', 'CN', 'FR'):\n",
    "    print('Language: %s' % lang)\n",
    "    print('Training...   ', end='')\n",
    "    train = load_file('%s/train' % lang)\n",
    "    emissions = emission_estimate2(train)\n",
    "    print('OK')\n",
    "    \n",
    "    print('Predicting... ', end='')\n",
    "    dev = load_file('%s/dev.in' % lang)\n",
    "    simple_analyse(emissions, dev, '%s/dev.p2.out' % lang)\n",
    "    print('OK')\n",
    "    \n",
    "    print('Analysing...  ', end='')\n",
    "    with open('%s/dev.p2.out' % lang) as f:\n",
    "        pred = get_entities(f)\n",
    "    with open('%s/dev.out' % lang) as f:\n",
    "        gold = get_entities(f)\n",
    "    print('OK')\n",
    "    \n",
    "    compare_result(gold, pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "### Estimating transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_estimate(train):\n",
    "    # count transition j -> i\n",
    "    counts_ji = defaultdict(int)\n",
    "    \n",
    "    # count occurence of states\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for line in train:\n",
    "        y_j = 'START'\n",
    "        counts['START'] += 1\n",
    "        \n",
    "        for tok in line:\n",
    "            y_i = tok[-1]\n",
    "            counts[y_i] += 1\n",
    "            counts_ji[(y_j, y_i)] += 1\n",
    "            y_j = y_i\n",
    "            \n",
    "        counts_ji[(y_j, 'STOP')] += 1\n",
    "        counts['STOP'] += 1\n",
    "    \n",
    "    q = {}\n",
    "    for y_i, count_y in counts.items():\n",
    "        for y_j in counts:\n",
    "            count_ji = counts_ji[(y_j, y_i)]\n",
    "            if count_ji:\n",
    "                q[(y_i, y_j)] = count_ji/count_y\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(e, q, sentence):\n",
    "    xs, T = tuple(set(i) for i in zip(*e.keys()))\n",
    "    T.update({'START', 'STOP'})\n",
    "    \n",
    "    pis = [{k: (0, None) for k in T}]\n",
    "    pis[0]['START'] = (1, None)\n",
    "    n = len(sentence)\n",
    "    \n",
    "    def val(i, u, v, word):\n",
    "        return pis[i][u][0] * q.get((v, u), 0) * e.get((word, v), 0)\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        word = word.lower()\n",
    "        if word not in xs:\n",
    "            word = '#UNK#'\n",
    "        p_i = {}\n",
    "        for v in T:\n",
    "            p_i[v] = max((val(i, u, v, word), u) for u in T)\n",
    "        pis.append(p_i)\n",
    "    \n",
    "    last_pi = max((pis[n][u][0]*q.get(('STOP', u), 0), u) for u in T)[1]\n",
    "    \n",
    "    tags = [last_pi]\n",
    "    for i in range(n-1):\n",
    "        p = pis[n-i][last_pi][1]\n",
    "        tags.append(p)\n",
    "        last_pi = p\n",
    "        \n",
    "    tags.reverse()\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ('EN', 'SG', 'CN', 'FR'):\n",
    "    print('Language: %s' % lang)\n",
    "    print('Training...   ', end='')\n",
    "    train = load_file('%s/train' % lang)\n",
    "    emissions = emission_estimate2(train)\n",
    "    transitions = transition_estimate(train)\n",
    "    print('OK')\n",
    "    \n",
    "    print('Predicting... ', end='')\n",
    "    dev = load_file('%s/dev.in' % lang)\n",
    "    with open('%s/dev.p3.out' % lang, 'w') as f:\n",
    "        for line in dev:\n",
    "            words = tuple(l[0] for l in line)\n",
    "            tags = viterbi(emissions, transitions, words)\n",
    "            for a in zip(words, tags):\n",
    "                print(' '.join(a), file=f)\n",
    "            print(file=f)\n",
    "    print('OK')\n",
    "    \n",
    "    print('Analysing...  ', end='')\n",
    "    with open('%s/dev.p3.out' % lang) as f:\n",
    "        pred = get_entities(f)\n",
    "    with open('%s/dev.out' % lang) as f:\n",
    "        gold = get_entities(f)\n",
    "    print('OK')\n",
    "    print()\n",
    "    compare_result(gold, pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "### Forward Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(e, q, sentence):\n",
    "    xs, T = tuple(set(i) for i in zip(*e.keys()))\n",
    "    T.update({'START', 'STOP'})\n",
    "    \n",
    "    alphas = [{u: q.get((u, 'START'), 0) for u in T}]\n",
    "    \n",
    "    def val(i, u, v, word):\n",
    "        return alphas[i][v] * q.get((u, v), 0) * e.get((word, v), 0)\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        word = word.lower()\n",
    "        if word not in xs:\n",
    "            word = '#UNK#'\n",
    "        a_j = {}\n",
    "        for u in T:\n",
    "            a_j[u] = sum(val(i, u, v, word) for v in T)\n",
    "        alphas.append(a_j)\n",
    "        \n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(e, q, sentence):\n",
    "    xs, T = tuple(set(i) for i in zip(*e.keys()))\n",
    "    T.update({'START', 'STOP'})\n",
    "    \n",
    "    betas = []\n",
    "    \n",
    "    def val(i, u, v, word):\n",
    "        return betas[i-1][v] * q.get((v, u), 0) * e.get((word, u), 0)\n",
    "    \n",
    "    for i, word in enumerate(reversed(sentence)):\n",
    "        word = word.lower()\n",
    "        if word not in xs:\n",
    "            word = '#UNK#'\n",
    "        b_i = {}\n",
    "        for u in T:\n",
    "            if i:\n",
    "                b_i[u] = sum(val(i, u, v, word) for v in T)\n",
    "            else:\n",
    "                b_i[u] = q.get(('STOP', u), 0)*e.get((word, u), 0)\n",
    "        betas.append(b_i)\n",
    "    \n",
    "    betas.reverse()\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training...   ', end='')\n",
    "train = load_file('EN/train')\n",
    "emissions = emission_estimate2(train)\n",
    "transitions = transition_estimate(train)\n",
    "xs, T = tuple(set(i) for i in zip(*emissions.keys()))\n",
    "T.update({'START', 'STOP'})\n",
    "print('OK')\n",
    "\n",
    "print('Predicting... ', end='')\n",
    "dev = load_file('EN/dev.in')\n",
    "with open('EN/dev.p4.out', 'w') as f:\n",
    "    for line in dev:\n",
    "        words = tuple(l[0] for l in line)\n",
    "        als = forward(emissions, transitions, words)\n",
    "        bts = backward(emissions, transitions, words)\n",
    "\n",
    "        tags = []\n",
    "        for i in range(len(words)):\n",
    "            y_m = max((als[i].get(u, 0)*bts[i].get(u, 0), u) for u in T)[1]\n",
    "            tags.append(y_m)\n",
    "\n",
    "        for a in zip(words, tags):\n",
    "            print(' '.join(a), file=f)\n",
    "        print(file=f)\n",
    "print('OK')\n",
    "    \n",
    "print('Analysing...  ', end='')\n",
    "with open('EN/dev.p4.out') as f:\n",
    "    pred = get_entities(f)\n",
    "with open('EN/dev.out') as f:\n",
    "    gold = get_entities(f)\n",
    "print('OK')\n",
    "print()\n",
    "compare_result(gold, pred)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
